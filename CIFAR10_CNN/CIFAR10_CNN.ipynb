{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, MaxPool2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if ModelCheckpoint callback is used and load the latest checkpoint\n",
    "def load_latest_checkpoint(model, callbacks):\n",
    "    # Check if ModelCheckpoint callback is present\n",
    "    checkpoint_callback = None\n",
    "    for callback in callbacks:\n",
    "        if isinstance(callback, tf.keras.callbacks.ModelCheckpoint):\n",
    "            checkpoint_callback = callback\n",
    "            break\n",
    "    \n",
    "    if checkpoint_callback is not None:\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_callback.filepath)\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        \n",
    "        if latest_checkpoint:\n",
    "            print(f\"Loading weights from the latest checkpoint: {latest_checkpoint}\")\n",
    "            model.load_weights(latest_checkpoint)\n",
    "        else:\n",
    "            print(\"No checkpoint found.\")\n",
    "    else:\n",
    "        print(\"No ModelCheckpoint callback found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "(X_train_unscaled, y_train), (X_test_unscaled, y_test) = cifar10.load_data()\n",
    "\n",
    "print(f\"X_train shape: {np.shape(X_train_unscaled)}\")\n",
    "print(f\"y_train shape: {np.shape(y_train)}\")\n",
    "print(f\"X_test shape: {np.shape(X_test_unscaled)}\")\n",
    "print(f\"y_test shape: {np.shape(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the normalize the data and convert targets to one hot encoding\n",
    "X_train_valid, X_test = X_train_unscaled/255.0, X_test_unscaled/255.0\n",
    "\n",
    "y_cat_train_valid = to_categorical(y_train)\n",
    "y_cat_test = to_categorical(y_test)\n",
    "\n",
    "#split the training dataset again to produce a validation set\n",
    "X_train, X_valid, y_cat_train, y_cat_valid = train_test_split(X_train_valid, y_cat_train_valid, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from the latest checkpoint: ./checkpoints\\model_checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "#build the model\n",
    "INPUT_SHAPE = (32,32,3)\n",
    "KERNEL_SIZE = (3,3)\n",
    "model = Sequential()\n",
    "\n",
    "#common to increase number of filters as we add convolutional layers (pooling reduces by 2 so we double filters)\n",
    "\n",
    "#conv layers\n",
    "model.add(Conv2D(filters=32, kernel_size = KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size = KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "#pooling\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "#droput\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#conv layers\n",
    "model.add(Conv2D(filters=64, kernel_size = KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size = KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "#pooling\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "#droput\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#conv layers\n",
    "model.add(Conv2D(filters=128, kernel_size = KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size = KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "#pooling\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "#droput\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#callbacks\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    './checkpoints/model_checkpoint.ckpt', \n",
    "    save_weights_only=True, \n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Example callbacks list\n",
    "callbacks = [checkpoint_callback]\n",
    "\n",
    "# load the last checkpoint\n",
    "load_latest_checkpoint(model, callbacks)\n",
    "\n",
    "#define metrics\n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 32, 32, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 4, 4, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                131136    \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 420,586\n",
      "Trainable params: 419,690\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#show the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1172/1172 [==============================] - 370s 314ms/step - loss: 0.5451 - accuracy: 0.8128 - precision: 0.8746 - recall: 0.7574 - val_loss: 0.6447 - val_accuracy: 0.7897 - val_precision: 0.8519 - val_recall: 0.7394\n",
      "Epoch 2/3\n",
      "1172/1172 [==============================] - 364s 310ms/step - loss: 0.5178 - accuracy: 0.8249 - precision: 0.8819 - recall: 0.7731 - val_loss: 0.6580 - val_accuracy: 0.7906 - val_precision: 0.8402 - val_recall: 0.7555\n",
      "Epoch 3/3\n",
      "1172/1172 [==============================] - 272s 232ms/step - loss: 0.4932 - accuracy: 0.8321 - precision: 0.8844 - recall: 0.7851 - val_loss: 0.5719 - val_accuracy: 0.8184 - val_precision: 0.8668 - val_recall: 0.7734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c870cfbe0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "num_epochs = 3\n",
    "model.fit(X_train, y_cat_train, \n",
    "          epochs=num_epochs, \n",
    "          validation_data=(X_valid,y_cat_valid), \n",
    "          batch_size=32,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6065 - accuracy: 0.8083 - precision: 0.8612 - recall: 0.7676\n",
      "accuracy: 0.8083000183105469\n",
      "precision: 0.8612139821052551\n",
      "recall: 0.7675999999046326\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model on the test set\n",
    "test_results = model.evaluate(X_test, y_cat_test)\n",
    "loss = test_results[0]\n",
    "accuracy = test_results[1]\n",
    "precision = test_results[2]\n",
    "recall = test_results[3]\n",
    "\n",
    "print(f'accuracy: {accuracy}')\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model and the test data for \n",
    "model.save('cifar10_CNN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
